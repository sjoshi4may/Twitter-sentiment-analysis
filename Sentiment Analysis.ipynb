{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv',encoding = 'ISO-8859-1').set_index('ItemID')\n",
    "test = pd.read_csv('test.csv',encoding = 'ISO-8859-1').set_index('ItemID')\n",
    "\n",
    "# data = pd.read_csv('//Users//sharadjoshi//Desktop//Programming:Data Science//Kaggle data science//UMICH SI650 - Sentiment Classification//training.txt',encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lowercase all data\n",
    "data['SentimentText']=data['SentimentText'].str.lower()\n",
    "test['SentimentText']=test['SentimentText'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "table = str.maketrans('','',string.punctuation)\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda x: x.translate(table))\n",
    "test['SentimentText'] = test['SentimentText'].apply(lambda x: x.translate(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "data['SentimentText'] = data['SentimentText'].apply(nltk.word_tokenize)\n",
    "test['SentimentText'] = test['SentimentText'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopword = stopwords.words('english')\n",
    "for word in ['ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','mightn','mustn',\n",
    " 'needn','shan','shouldn','wasn','weren','wouldn','don','no','nor','not','won']:\n",
    "    stopword.remove(word)\n",
    "data['SentimentText']=data['SentimentText'].apply(lambda x: [word for word in x if word not in stopword])\n",
    "test['SentimentText']=test['SentimentText'].apply(lambda x: [word for word in x if word not in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "ps=PorterStemmer()\n",
    "data['SentimentText']=data['SentimentText'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "test['SentimentText']=test['SentimentText'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemID\n",
       "1         None\n",
       "2         None\n",
       "3         None\n",
       "4         None\n",
       "5         None\n",
       "6         None\n",
       "7         None\n",
       "8         None\n",
       "9         None\n",
       "10        None\n",
       "11        None\n",
       "12        None\n",
       "13        None\n",
       "14        None\n",
       "15        None\n",
       "16        None\n",
       "17        None\n",
       "18        None\n",
       "19        None\n",
       "20        None\n",
       "21        None\n",
       "22        None\n",
       "23        None\n",
       "24        None\n",
       "25        None\n",
       "26        None\n",
       "27        None\n",
       "28        None\n",
       "29        None\n",
       "30        None\n",
       "          ... \n",
       "99971     None\n",
       "99972     None\n",
       "99973     None\n",
       "99974     None\n",
       "99975     None\n",
       "99976     None\n",
       "99977     None\n",
       "99978     None\n",
       "99979     None\n",
       "99980     None\n",
       "99981     None\n",
       "99982     None\n",
       "99983     None\n",
       "99984     None\n",
       "99985     None\n",
       "99986     None\n",
       "99987     None\n",
       "99988     None\n",
       "99989     None\n",
       "99990     None\n",
       "99991     None\n",
       "99992     None\n",
       "99993     None\n",
       "99994     None\n",
       "99995     None\n",
       "99996     None\n",
       "99997     None\n",
       "99998     None\n",
       "99999     None\n",
       "100000    None\n",
       "Name: SentimentText, Length: 99989, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words=[]\n",
    "data['SentimentText'].apply(lambda x: all_words.extend(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[miss, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[omg, alreadi, 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cri, ive, dentis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[think, mi, bf, cheat, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SentimentText\n",
       "ItemID                                                   \n",
       "1                                      [sad, apl, friend]\n",
       "2                              [miss, new, moon, trailer]\n",
       "3                                     [omg, alreadi, 730]\n",
       "4       [omgaga, im, sooo, im, gunna, cri, ive, dentis...\n",
       "5                              [think, mi, bf, cheat, tt]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tweets'] = list(zip(data['SentimentText'],data['Sentiment']))\n",
    "# test = data.iloc[1:30000]\n",
    "# data = data.iloc[30000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_features = nltk.FreqDist(all_words)\n",
    "word_features_small = sorted(word_features,key=word_features.get,reverse=True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features_small:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_Set = nltk.classify.apply_features(extract_features, list(data['tweets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier.show_most_informative_features(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Sentiment'] = test['SentimentText'].apply(lambda x: classifier.classify(extract_features(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['ItemID','Sentiment']].to_csv('solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cm = test['Sentiment']-y_pred\n",
    "# cm[cm==0].shape[0]/(cm[cm==0].shape[0]+cm[cm!=0].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
